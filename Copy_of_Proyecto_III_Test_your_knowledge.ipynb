{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DayanaSanchez12/Digitales-proyecto-calculadora/blob/main/Copy_of_Proyecto_III_Test_your_knowledge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh9FSqPPCy2w"
      },
      "source": [
        "# Proyecto III:  Test your knowledge\n",
        "\n",
        "En este proyecto final para la asignatura Inteligencia Artificial. El objetivo es que ustedes prueben todos sus conocimientos en el desarrollo de sistemas de clasificación o regresión utilizando redes neuronales artificiales clásicas. Para este proyecto ustedes solucionaran un problema de regresión o clasificación del [UCI Repository](https://archive.ics.uci.edu/ml/index.php). La idea es que los estudiantes se dividan en grupos de dos personas, y del listado de problemas escojan un problema para solucionarlo. En cada uno de los problemas los estudiantes deberán proporcionar los siguientes elementos:\n",
        "\n",
        "1. Explicación básica del problema.\n",
        "2. Un notebook con el desarrollo del problema:\n",
        "    * Tratamiento de los datos,\n",
        "    * Analisis exploratorio de datos\n",
        "    * Esquema de entrenamiento.\n",
        "    * Esquema de prueba de funcionamiento del modelo\n",
        "    * Evaluación de VARIOS modelos de clasificación o regresión, generación de intervalos de confidencia para la salida de regresión (Estimación, errores, etc..) o clasificiación (medidas de performance como accuracy, sensitivity, AUC, curva ROC, matriz de confusión, etc... con sus respectivos intervalos de confidencia).\n",
        "    \n",
        "3. El notebook con una discusión a fondo del comportamiento de los modelos incluyendo los siguentes aspectos, pero no limitado a ellos:\n",
        "    * Discusion a fondo de los resultados, si hay un clasificador de varias clases, ¿En prueba se comporta igual para todas las clases? ¿Si no se comporta igual esto a que se debe?, En regresión ¿en que partes los intervalos de confidencia son menores?, ¿porque?. Asi con los demás puntos que ustedes quieran discutir.\n",
        "    * Discusión sobre que información pueden obtener a partir de los resultados que entrega el modelo. (¿que varaibles son más importantes? ¿Cuando funciona mejor/peor?, etc..)\n",
        "    * Discusión a fondo del comportamiento de los modelos encontrados, posibles fallas, posibles mejoras.\n",
        "    \n",
        "4. Una reflexión sobre lo aprendido del curso, ¿cubrio sus espectativas?, ¿Su visión de Inteligencia artificial cambió o se mantuvo igual?, ¿Qué le mejoraria al curso?, ¿Cual fue su tema favorito y menos favorito? y otros comentarios que deseen agregar.\n",
        "\n",
        "\n",
        "Los problemas a solucionar son los siguientes:\n",
        "\n",
        "**Clasificación I: Wine Quality**, desarrolle un clasificador que permita identificar la calidad del vino. La información la encuentran [aquí.](https://archive.ics.uci.edu/ml/datasets/Wine+Quality)\n",
        "\n",
        "**Clasificación II: Bank Marketing**, desarrollar un sistema de clasificación para determinar si un cliente inscribira un deposito a término o no. La información la pueden encontrar [aquí.](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing)\n",
        "\n",
        "**Regresión I: Estimando el Tráfico en Sao Paulo**, desarrolar un modelo de regresión que permita identificar cual es la lentitud del trafico en Sao Paulo, Brazil, la información la pueden encontrar [aquí.](https://archive.ics.uci.edu/ml/datasets/Behavior+of+the+urban+traffic+of+the+city+of+Sao+Paulo+in+Brazil)\n",
        "\n",
        "**Regresión II: Estimando demanda de energia de una casa**, desarrollar un modelo de regresión que permita predecir el consumo de energía en una casa. La información la pueden encontrar [aquí.](https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption)\n",
        "\n",
        "\n",
        "**NOTA:** Si desean trabajar en otro problema que se encuentre en este repositorio me pueden indicar su interes y evaluamos si se puede desarrollar.\n",
        "\n",
        "\n",
        "**El proyecto se debe entregar el Viernes 24 de Noviembre, a más tardar las 11:59 p.m.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PROYECTO FINAL INTELIGENCIA ARTIFICAL - 2023-3\n",
        "##JASMIN DAYANA SANCHEZ GUEVARA - MIGUEL AUGUSTO PARADA HOYOS\n",
        "##PROFESOR ALEXANDER CAICEDO DORADO\n",
        "##PONTIFICIA UNIVERSIDAD JAVERIANA"
      ],
      "metadata": {
        "id": "lS63UZMuZF5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXPLICACIÓN BÁSICA DEL PROBLEMA\n",
        "##1.1 Planteamiento del Problema\n",
        " Con propósito de realizar un estudio de la eficiencia de diferentes aproximaciones en inteligencia artificial para realizar un modelo de regresión que permita identificar cual es la lentitud del trafico en Sao Paulo, Brazil; se implementaran diferentes técnicas de regresión por medio de un modelo lineal, maquinas de soporte vectorial con diferentes kernel y redes neuronales.\n",
        "\n",
        " El conjunto de datos \"Behavior of the Urban Traffic of the City of Sao Paulo\" representa un análisis exhaustivo del tráfico urbano en una de las ciudades más grandes y congestionadas del mundo, São Paulo, Brasil. Durante una semana laboral en diciembre de 2009, se recogieron datos cada 30 minutos, desde las 7:00 hasta las 20:00, capturando así los patrones de tráfico en diferentes momentos del día. Este conjunto de datos incluye varias variables que se cree afectan directa o indirectamente a la fluidez del tráfico. Estas variables van desde incidentes obvios que podrían causar retrasos, como autobuses inmovilizados, camiones averiados, y accidentes, hasta factores menos evidentes pero igualmente importantes como manifestaciones, problemas en la red de trolebuses, y semáforos apagados.\n",
        "\n",
        "La variable objetivo, el porcentaje de lentitud en el tráfico, ofrece una medida cuantitativa del impacto de estos factores en la fluidez del tráfico. El desafío es desarrollar un modelo de regresión capaz de predecir esta lentitud en el tráfico, lo que no solo implica entender la relación entre los diferentes factores y la lentitud en el tráfico, sino también enfrentar el desafío de modelar patrones de tráfico que pueden ser altamente variables y dependientes del contexto. La relevancia de este análisis radica en su potencial para informar políticas de planificación urbana y estrategias de gestión del tráfico, posiblemente contribuyendo a soluciones para reducir la congestión y mejorar la movilidad en áreas urbanas densamente pobladas.\n",
        "\n",
        "##1.2 Justificación del Estudio\n",
        "El estudio sobre la eficiencia de diversas aproximaciones en inteligencia artificial para modelar la lentitud del tráfico en Sao Paulo, Brazil, se justifica por la creciente importancia de comprender y abordar los desafíos relacionados con la movilidad urbana. La congestión del tráfico no solo afecta la eficiencia y el tiempo de viaje de los ciudadanos, sino que también tiene ramificaciones económicas, ambientales y sociales. Mediante la aplicación de técnicas avanzadas de regresión, como modelos lineales, máquinas de soporte vectorial y redes neuronales, se busca no solo predecir la lentitud del tráfico de manera precisa, sino también identificar qué enfoques de inteligencia artificial son más efectivos para abordar esta problemática específica.\n",
        "\n",
        "##1.3 Objetivos del Estudio\n",
        "\n",
        "El objetivo principal de este estudio es desarrollar un modelo de regresión robusto que pueda prever con precisión la lentitud del tráfico en Sao Paulo, Brasil. Para lograr esto, se plantean los siguientes objetivos específicos:\n",
        "\n",
        "*  Implementar y comparar un modelo de regresión lineal para evaluar su capacidad predictiva en el contexto del comportamiento del tráfico urbano.\n",
        "*  Explorar y analizar el rendimiento de máquinas de soporte vectorial con diferentes kernels, evaluando su capacidad para capturar patrones complejos en los datos de tráfico.  \n",
        "*  Desarrollar y evaluar una red neuronal para modelar la lentitud del tráfico, aprovechando la capacidad de las redes neuronales para aprender representaciones no lineales de los datos.\n",
        "\n",
        "A través de estos objetivos, se busca no solo construir un modelo preciso, sino también entender cómo diferentes enfoques de inteligencia artificial se adaptan a la complejidad del comportamiento del tráfico en una ciudad dinámica como Sao Paulo.\n"
      ],
      "metadata": {
        "id": "ZO_8v1VsZ46v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DESARROLLO DEL PROBLEMA\n",
        "Antes de realizar cualquier aproximación a los modelos de regresión por cualqueira de las técnicas, realizamos un análisis previo de los datos que nos permitirá tener un mejor críterio en las decisiones propias a cada modelo, como la selección del kernel para SVM. Además en el caso de SVM, los kernels escogidos son lineal y polinómico."
      ],
      "metadata": {
        "id": "WRIZ7Wnrcw2X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPpMrRPqCy2z"
      },
      "outputs": [],
      "source": [
        "# Importando las librerias necesarias\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "from scipy.io import arff\n",
        "\n",
        "# Cargamos el dataset\n",
        "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00477/Behavior%20of%20the%20urban%20traffic%20of%20the%20city%20of%20Sao%20Paulo%20in%20Brazil.arff\"\n",
        "data, meta = arff.loadarff(data_url)\n",
        "dataset = pd.DataFrame(data)\n",
        "# Preparación de datos: Remover outliers\n",
        "def remove_outliers(df, columns, threshold=3):\n",
        "    z_scores = np.abs(stats.zscore(df[columns]))\n",
        "    df_no_outliers = df[(z_scores < threshold).all(axis=1)]\n",
        "    return df_no_outliers\n",
        "\n",
        "# Seleccionamos las columnas para remover outliers\n",
        "outlier_columns = ['Immobilized_bus', 'Broken_Truck', 'Vehicle_excess', 'Accident_victim', 'Running_over',\n",
        "                   'Fire_vehicles', 'Occurrence_involving_freight', 'Incident_involving_dangerous_freight',\n",
        "                   'Lack_of_electricity', 'Fire', 'Point_of_flooding', 'Manifestations',\n",
        "                   'Defect_in_the_network_of_trolleybuses', 'Tree_on_the_road', 'Semaphore_off', 'Intermittent_Semaphore']\n",
        "\n",
        "# Removemos outliers\n",
        "dataset = remove_outliers(dataset, outlier_columns)\n",
        "\n",
        "# Visualizamos los primeros datos en formato tabla después de remover outliers\n",
        "print(\"Dataset after removing outliers:\")\n",
        "print(dataset.head())\n",
        "\n",
        "# Crear una tabla para almacenar resultados\n",
        "results_table = pd.DataFrame(columns=['Bootstrap_Sample', 'Kernel', 'MSE', 'MAE'])\n",
        "\n",
        "# Seleccionamos las variables dependientes e independientes\n",
        "X = dataset.drop('Slowness_in_traffic_percent', axis=1)\n",
        "y = dataset['Slowness_in_traffic_percent']\n",
        "\n",
        "# Normalizamos las variables independientes\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Iterar sobre diferentes tamaños de bootstrap\n",
        "for bootstrap_size in range(1, 11):\n",
        "    # Realizamos la división en datos de entrenamiento y de prueba\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "    # Implementamos el bootstraping\n",
        "    X_train, y_train = resample(X_train, y_train, replace=True, random_state=bootstrap_size * 10)\n",
        "\n",
        "    # Creamos e implementamos la regresión SVM con un núcleo lineal\n",
        "    svm_linear = svm.SVR(kernel='linear')\n",
        "    svm_linear.fit(X_train, y_train)\n",
        "\n",
        "    # Predicciones en el conjunto de prueba\n",
        "    y_pred = svm_linear.predict(X_test)\n",
        "\n",
        "    # Calculamos MSE y MAE en el conjunto de prueba\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    # Agregamos resultados a la tabla\n",
        "    results_table = results_table.append({'Bootstrap_Sample': bootstrap_size,\n",
        "                                          'Kernel': 'Linear',\n",
        "                                          'MSE': mse,\n",
        "                                          'MAE': mae}, ignore_index=True)\n",
        "\n",
        "    # Creamos e implementamos la regresión SVM con un núcleo polinómico\n",
        "    svm_poly = svm.SVR(kernel='poly', degree=2)\n",
        "    svm_poly.fit(X_train, y_train)\n",
        "\n",
        "    # Predicciones en el conjunto de prueba\n",
        "    y_pred_poly = svm_poly.predict(X_test)\n",
        "\n",
        "    # Calculamos MSE y MAE en el conjunto de prueba\n",
        "    mse_poly = mean_squared_error(y_test, y_pred_poly)\n",
        "    mae_poly = mean_absolute_error(y_test, y_pred_poly)\n",
        "\n",
        "    # Agregamos resultados a la tabla\n",
        "    results_table = results_table.append({'Bootstrap_Sample': bootstrap_size,\n",
        "                                          'Kernel': 'Poly',\n",
        "                                          'MSE': mse_poly,\n",
        "                                          'MAE': mae_poly}, ignore_index=True)\n",
        "\n",
        "# Calcular intervalos y medias para MSE y MAE\n",
        "mse_interval = (results_table['MSE'].min(), results_table['MSE'].max())\n",
        "mae_interval = (results_table['MAE'].min(), results_table['MAE'].max())\n",
        "mse_mean = results_table['MSE'].mean()\n",
        "mae_mean = results_table['MAE'].mean()\n",
        "\n",
        "# Mostrar resultados\n",
        "print(\"\\nResults Table:\")\n",
        "print(results_table)\n",
        "\n",
        "print(\"\\nMSE Interval: \", mse_interval)\n",
        "print(\"MAE Interval: \", mae_interval)\n",
        "print(\"MSE Mean: \", mse_mean)\n",
        "print(\"MAE Mean: \", mae_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se plantea primero el desarrollo por maquinas de soporte vectorial. Los kernels seleccionados para la problematica del tráfico"
      ],
      "metadata": {
        "id": "8fZC8Y5kgrGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "En este entorno de Jupyter, se realizó un análisis de datos exhaustivo, comenzando con la importación de bibliotecas esenciales como pandas, numpy y PyTorch. Luego, se cargó y limpió un conjunto de datos, asegurando que estuviera en formato numérico y listo para su procesamiento.\n",
        "\n",
        "La preparación de los datos incluyó la separación de características y la variable objetivo, que en este caso era la lentitud del tráfico. Se definió una red neuronal utilizando PyTorch con capas lineales y funciones de activación ReLU para el modelado.\n",
        "\n",
        "El proceso de evaluación involucró la división de datos en subconjuntos, normalización, y el entrenamiento y evaluación de modelos de regresión lineal y redes neuronales. Además, se aplicó el método de Bootstrap para evaluar la robustez de los modelos mediante múltiples iteraciones.\n",
        "\n",
        "Finalmente, se calculó la media y el intervalo de confianza del Error Cuadrático Medio (MSE) para ambos modelos, y se visualizaron los resultados mediante histogramas. Este enfoque integral permitió un análisis detallado y sólido del conjunto de datos y los modelos de predicción."
      ],
      "metadata": {
        "id": "5FCyaNjrqgHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.utils import resample\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Cargar y limpiar los datos\n",
        "file_path = 'Behavior of the urban traffic of the city of Sao Paulo in Brazil.csv'\n",
        "df = pd.read_csv(file_path, sep=';')\n",
        "df = df.replace(',', '.', regex=True).astype(float)\n",
        "\n",
        "# Preparación de los datos\n",
        "X = df.drop('Slowness in traffic (%)', axis=1).values\n",
        "y = df['Slowness in traffic (%)'].values\n",
        "\n",
        "# Definir la red neuronal\n",
        "class RegressionNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RegressionNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(X.shape[1], 100)\n",
        "        self.fc2 = nn.Linear(100, 500)\n",
        "        self.fc3 = nn.Linear(500, 1000)\n",
        "        self.fc4 = nn.Linear(1000, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# División en subconjuntos y preparación para entrenamiento\n",
        "kf = KFold(n_splits=20, shuffle=True, random_state=42)\n",
        "resultados = []\n",
        "\n",
        "for train_index, test_index in kf.split(df):\n",
        "    df_train, df_test = df.iloc[train_index], df.iloc[test_index]\n",
        "    # Normalización de los datos\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(df_train.drop('Slowness in traffic (%)', axis=1))\n",
        "    X_test_scaled = scaler.transform(df_test.drop('Slowness in traffic (%)', axis=1))\n",
        "    y_train = df_train['Slowness in traffic (%)'].values\n",
        "    y_test = df_test['Slowness in traffic (%)'].values\n",
        "\n",
        "    # Convertir a tensores de PyTorch para la red neuronal\n",
        "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
        "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32)\n",
        "\n",
        "    # Instanciar y entrenar el modelo de red neuronal\n",
        "    model_nn = RegressionNN()\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model_nn.parameters(), lr=0.01)\n",
        "    epochs = 1000\n",
        "    for epoch in range(epochs):\n",
        "        model_nn.train()\n",
        "        outputs = model_nn(X_train_tensor)\n",
        "        loss = criterion(outputs, y_train_tensor)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluación del modelo de red neuronal\n",
        "    model_nn.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred_nn = model_nn(X_test_tensor).numpy()\n",
        "\n",
        "    # Entrenamiento y evaluación del modelo de regresión lineal\n",
        "    model_linear = LinearRegression()\n",
        "    model_linear.fit(X_train_scaled, y_train)\n",
        "    y_pred_linear = model_linear.predict(X_test_scaled)\n",
        "\n",
        "    # Almacenar resultados\n",
        "    mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
        "    mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
        "    resultados.append((mse_nn, mse_linear))\n",
        "\n",
        "    # Número de iteraciones de bootstrap\n",
        "    n_iterations = 1000\n",
        "    mse_scores_linear, mse_scores_nn = [], []\n",
        "\n",
        "for i in range(n_iterations):\n",
        "    # Muestreo con reemplazo para bootstrap\n",
        "    X_sample, y_sample = resample(X_train_scaled, y_train)\n",
        "\n",
        "    # Entrenar y evaluar el modelo de regresión lineal\n",
        "    model_linear.fit(X_sample, y_sample)\n",
        "    y_pred_linear = model_linear.predict(X_test_scaled)\n",
        "    mse_scores_linear.append(mean_squared_error(y_test, y_pred_linear))\n",
        "\n",
        "    # Preparar datos para PyTorch\n",
        "    X_sample_tensor = torch.tensor(X_sample, dtype=torch.float32)\n",
        "    y_sample_tensor = torch.tensor(y_sample.reshape(-1, 1), dtype=torch.float32)\n",
        "\n",
        "    # Entrenar el modelo de red neuronal\n",
        "    for epoch in range(epochs):\n",
        "        model_nn.train()\n",
        "        outputs = model_nn(X_sample_tensor)\n",
        "        loss = criterion(outputs, y_sample_tensor)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluar el modelo de red neuronal\n",
        "    model_nn.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred_nn = model_nn(X_test_tensor).numpy()\n",
        "    mse_scores_nn.append(mean_squared_error(y_test, y_pred_nn))\n",
        "\n",
        "\n",
        "# Calcular la media y el intervalo de confianza para MSE de ambos modelos\n",
        "mse_ci_linear = np.percentile(mse_scores_linear, [2.5, 97.5])\n",
        "mse_ci_nn = np.percentile(mse_scores_nn, [2.5, 97.5])\n",
        "\n",
        "# Visualización de los resultados del bootstrap\n",
        "plt.figure(figsize=(15, 7))\n",
        "\n",
        "# Gráfica para la regresión lineal\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(mse_scores_linear, bins=30, alpha=0.7, label='MSE Linear')\n",
        "plt.axvline(np.mean(mse_scores_linear), color='r', linestyle='-', label='Media MSE')\n",
        "plt.axvline(mse_ci_linear[0], color='g', linestyle='--', label='IC bajo')\n",
        "plt.axvline(mse_ci_linear[1], color='b', linestyle='--', label='IC alto')\n",
        "plt.xlabel('MSE')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.title('Distribución de MSE - Regresión Lineal')\n",
        "plt.legend()\n",
        "\n",
        "# Gráfica para la red neuronal\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(mse_scores_nn, bins=30, alpha=0.7, label='MSE NN')\n",
        "plt.axvline(np.mean(mse_scores_nn), color='r', linestyle='-', label='Media MSE')\n",
        "plt.axvline(mse_ci_nn[0], color='g', linestyle='--', label='IC bajo')\n",
        "plt.axvline(mse_ci_nn[1], color='b', linestyle='--', label='IC alto')\n",
        "plt.xlabel('MSE')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.title('Distribución de MSE - Red Neuronal')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "itPnxm35nqDi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}